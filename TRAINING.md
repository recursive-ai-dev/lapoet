# AG-TUNE Training Documentation

## Overview

AG-TUNE includes a comprehensive training system that learns from lyrics and poetry to generate new poetic content. The training process builds vocabulary, computes word embeddings, trains a Kernel PCA for emotional space, and optimizes a TD-lambda value estimator.

## Training Architecture

### Components

1. **Vocabulary Building**: Extracts all unique words from the training corpus
2. **Co-occurrence Embeddings**: Creates context-based word vectors using sliding window
3. **Kernel PCA**: Transforms embeddings into 8-dimensional emotional/thematic space
4. **TD Value Estimator**: Reinforcement learning component that learns aesthetic values

### Training Process

The training loop:
1. Loads lyrics from the `lyrics/` folder (847+ lines from 15 files)
2. Tokenizes and builds a vocabulary of ~1,675 unique words
3. Computes co-occurrence embeddings with a 3-word context window
4. Trains polynomial Kernel PCA (degree=3) on embeddings
5. Runs 100 epochs of TD-lambda learning to optimize aesthetic value estimation
6. Saves a checkpoint with all learned parameters

## Running Training

### Prerequisites

```bash
npm install
```

### Train on Lyrics

```bash
npm run train
```

This will:
- Load all lyrics from `lyrics/*.txt`
- Train the model for 100 epochs
- Save checkpoint to `agtune-lyrics-checkpoint.json`
- Display training metrics and verification

### Verify Information Retention

```bash
npm run verify
```

This runs comprehensive tests to verify:
- Multiple save/load cycles preserve data
- Emotional space vectors remain stable
- TD weights are correctly persisted
- Vocabulary completeness (99%+ coverage)
- Data integrity across cycles

## Using the Trained Model

### In Web UI

1. Start the development server:
```bash
npm run dev
```

2. Open http://localhost:5173 in your browser

3. Click "Load Checkpoint" and upload `agtune-lyrics-checkpoint.json`

4. The model is now ready to generate poetry based on learned lyrics

### Command Line

You can load the checkpoint programmatically:

```javascript
import { AGTuneEngine } from './src/App.jsx';

const engine = new AGTuneEngine();
engine.loadCheckpoint(checkpointData);

// Now use engine.generatePoem() with trained knowledge
```

## Training Metrics

### Expected Results

After 100 epochs of training:
- **Vocabulary Size**: ~1,675 words
- **Average Reward**: ~4.8
- **Emotional Vectors**: 1,675 (one per word)
- **TD Weights**: 16 dimensions
- **Coverage**: 99%+ of lyrics vocabulary

### What the Model Learns

1. **Word Relationships**: Which words appear together in context
2. **Emotional Trajectories**: How words map to emotional/thematic dimensions
3. **Aesthetic Value**: What makes a line or sequence poetically valuable
4. **Pattern Recognition**: Common structures and rhythms in the lyrics

## Checkpoint Format

The checkpoint file (`agtune-lyrics-checkpoint.json`) contains:

```json
{
  "vocabulary": [[word, frequency], ...],
  "embeddings": [[word, [vector]], ...],
  "emotionalSpace": [[word, [emotional_vector]], ...],
  "kernelPCA": {
    "nComponents": 8,
    "degree": 3,
    "eigenvectors": [...],
    "eigenvalues": [...],
    "X_fit": [...]
  },
  "valueEstimator": {
    "weights": [...],
    "alpha": 0.01,
    "gamma": 0.95,
    "lambda": 0.8
  },
  "lastCorpusSignature": "hash",
  "isTrained": true,
  "timestamp": "ISO date"
}
```

## Indefinite Retention

The model retains information indefinitely through:

1. **Persistent Checkpoints**: All learned parameters saved to JSON
2. **Lossless Serialization**: Full precision floating-point storage
3. **Verification Tests**: Automated tests ensure data integrity
4. **Multiple Save/Load Cycles**: Tested across 3+ cycles with perfect retention

### Proof of Retention

Run the verification script to see proof:

```bash
npm run verify
```

This will demonstrate:
- âœ“ All word vectors preserved exactly
- âœ“ TD weights unchanged after multiple cycles
- âœ“ Vocabulary 99%+ complete
- âœ“ Checksums match across saves/loads

## Continuous Training

To continue training an existing checkpoint:

1. Load the checkpoint into a new engine instance
2. Call `engine.train(newCorpus, epochs)` with additional data
3. The model will incrementally update without losing prior knowledge
4. Save a new checkpoint with the updated parameters

### Example: Adding New Lyrics

```javascript
const engine = new AGTuneEngine();
engine.loadCheckpoint(existingCheckpoint);

// Add new lyrics
const newLyrics = ["new verse 1", "new verse 2"];
engine.train(newLyrics, 50);

// Save updated model
engine.saveCheckpoint('updated-checkpoint.json');
```

## Training Parameters

### Tunable Hyperparameters

- **Epochs**: Number of training iterations (default: 100)
- **Kernel Degree**: Polynomial kernel degree (default: 3)
- **N Components**: Emotional space dimensions (default: 8)
- **TD Alpha**: Learning rate (default: 0.01)
- **TD Gamma**: Discount factor (default: 0.95)
- **TD Lambda**: Eligibility trace decay (default: 0.8)
- **Context Window**: Co-occurrence window size (default: 3)

### Recommended Values

For lyrics training:
- Epochs: 100-200 (more epochs = better convergence)
- Kernel Degree: 3 (captures non-linear relationships)
- N Components: 8 (balances expressiveness and computation)
- TD Alpha: 0.01 (stable learning without overshooting)

## Performance

### Training Time

On typical hardware:
- **Vocabulary Building**: < 1 second
- **Embedding Computation**: < 2 seconds
- **Kernel PCA**: 2-5 seconds
- **TD Training (100 epochs)**: 10-20 seconds
- **Total**: ~20-30 seconds

### Memory Usage

- **Checkpoint Size**: ~1.7 MB for 1,675 words
- **Training Memory**: ~50-100 MB during training phase
- **Loaded Model Memory**: ~30-50 MB in browser after loading checkpoint

## Troubleshooting

### Training Fails

- Ensure lyrics files are in `lyrics/` folder
- Check file encoding is UTF-8
- Verify at least 10+ lines of text per file

### Low Coverage

- Add more lyrics files to increase vocabulary
- Ensure lyrics contain varied vocabulary
- Check for filtering that removes valid words

### Poor Generation Quality

- Train for more epochs (150-200)
- Add more diverse training data
- Adjust beam width during generation (try 7-10)

## Advanced Usage

### Custom Training Loop

```javascript
const engine = new AGTuneEngine();

// Load and preprocess lyrics
const lyrics = loadLyricsFromFiles();

// Train with custom parameters
for (let epoch = 0; epoch < 200; epoch++) {
  const reward = engine.train(lyrics, 1);
  console.log(`Epoch ${epoch}: ${reward}`);
  
  // Save checkpoint every 50 epochs
  if (epoch % 50 === 0) {
    engine.saveCheckpoint(`checkpoint-epoch-${epoch}.json`);
  }
}
```

### Batch Training

For large datasets, train in batches:

```javascript
const batches = splitIntoBatches(lyrics, 100);

for (const batch of batches) {
  engine.train(batch, 10);
  console.log(`Batch complete, vocab: ${engine.vocabulary.size}`);
}
```

## Citation

If you use the AG-TUNE training system in your research, please cite:

```
Davison, D., Maillet, M., & Davison, S. (2025).
AG-TUNE: A Neuro-Symbolic Generative Poetry Engine.
Recursive AI Devs. https://github.com/recursive-ai-dev/lapoet
```

## License

Apache-2.0 License - See LICENSE file for details.

---

## Quick Reference

```bash
# Train model on lyrics
npm run train

# Verify retention
npm run verify

# Test engine components
npm run test-engine

# Start web UI
npm run dev
```

**Training complete! The model now retains lyrics information indefinitely.** ðŸŽ¯
